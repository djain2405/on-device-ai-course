**üìò Module 3 - Core ML on iOS (Hands-On)**

Build an On-Device Image Classifier with Core ML + SwiftUI

Welcome to Module 3 - the iOS counterpart to your TensorFlow Lite module.

In this module, you‚Äôll build a fully on-device image classifier in SwiftUI using Core ML.
You‚Äôll learn how to load a model, convert UIImage into a CVPixelBuffer, run inference locally, and build a friendly UI to show predictions and confidence.

This module is deliberately simple - everything you build here will become the foundation for later modules on privacy patterns, behavioral prediction, and on-device LLMs.

üéØ **Learning Goals**

After completing this module, you will:

* Understand the Core ML pipeline for image models

* Convert images into the format the model expects

* Load and run a .mlmodel file

* Build a SwiftUI app that performs inference on-device

* Display predictions and confidence with nice UX

* Understand iOS-specific ML performance behavior

* Run classification under ~50ms on recent devices


üß© **1.What Is Core ML?**

Core ML is Apple‚Äôs framework for running machine learning models directly on iOS, macOS, watchOS, and tvOS.

It provides:

‚úî Extremely fast inference (thanks to the Neural Engine)

‚úî Automatic model compilation

‚úî Tight integration with Create ML and Xcode

‚úî Privacy-by-design (data stays on-device)

For image classification, Core ML expects:

* a compiled model (.mlmodelc)

* an input pixel buffer in the expected size

* a prediction request returning label probabilities

The nice part?
Much of the boilerplate is handled automatically.

üìÅ **2. Starter Project Structure**

```
starter/
   OnDevideClassifierDemo/
      ContentView.swift
      ImagePicker.swift
      Models/
         MobileNetV2.mlmodel
      Assets/

```

The starter has:

* Photo picker

* Basic SwiftUI layout

* Core ML model included

* Placeholder UI awaiting your logic

üß† **3. Adding the MobileNetV2 Model**

Apple provides an excellent MobileNetV2 model via the Xcode model library.

You can download it here:
https://developer.apple.com/machine-learning/models/

Or add it manually via:

File ‚Üí Add Files to ‚ÄúOnDevideClassifierDemo‚Ä¶‚Äù


Xcode will:

* validate

* compile

* autogenerate Swift API bindings

This is why Core ML feels magical.

üîç **4. Understanding the Core ML Pipeline**

The flow is:

```UIImage ‚Üí CVPixelBuffer ‚Üí model.prediction(...) ‚Üí label + confidence```


The app converts the selected UIImage directly to CIImage (Core Image format)

The only tricky part is the pixel buffer conversion.

The Vision framework (VNImageRequestHandler) automatically:
* Converts the CIImage to a CVPixelBuffer
* Resizes the image to match MobileNetV2's input requirements (224x224)
* Normalizes pixel values to the expected range
* Handles color space conversions (RGB)

üß† **5. Code Walkthrough**

Classify function

```
import SwiftUI
import Vision
import CoreML

    private func classify(_ uiImage: UIImage?) {
        guard let ciImage = CIImage(image: uiImage ?? UIImage()) else { return }
        guard let model = try? VNCoreMLModel(for: MobileNetV2(configuration: MLModelConfiguration()).model) else { return }

        let request = VNCoreMLRequest(model: model) { req, _ in
            if let result = req.results?.first as? VNClassificationObservation {
                DispatchQueue.main.async {
                    label = "\(result.identifier) (\(Int(result.confidence * 100))%)"
                }
            }
        }

        try? VNImageRequestHandler(ciImage: ciImage).perform([request])
    }

```
This uses the autogenerated MobileNetV2 API - no Vision wrappers needed for this simple task.

üé® **6. Building the SwiftUI UI**

ContentView.swift

```
import SwiftUI
import Vision
import CoreML

struct ContentView: View {
    @State private var image: UIImage?
    @State private var label: String = "Pick an image to classify üê±üê∂"
    @State private var showPicker = false

    var body: some View {
        VStack(spacing: 20) {
            if let image = image {
                Image(uiImage: image)
                    .resizable()
                    .scaledToFit()
                    .frame(height: 300)
                    .clipShape(RoundedRectangle(cornerRadius: 12))
            }

            Text(label)
                .font(.headline)
                .multilineTextAlignment(.center)
                .padding()

            Button("Choose Photo") { showPicker = true }
                .buttonStyle(.borderedProminent)
        }
        .sheet(isPresented: $showPicker) {
            ImagePicker(selectedImage: $image) { uiImage in
                classify(uiImage)
            }
        }
        .padding()
    }

    private func classify(_ uiImage: UIImage?) {
        guard let ciImage = CIImage(image: uiImage ?? UIImage()) else { return }
        guard let model = try? VNCoreMLModel(for: MobileNetV2(configuration: MLModelConfiguration()).model) else { return }

        let request = VNCoreMLRequest(model: model) { req, _ in
            if let result = req.results?.first as? VNClassificationObservation {
                DispatchQueue.main.async {
                    label = "\(result.identifier) (\(Int(result.confidence * 100))%)"
                }
            }
        }

        try? VNImageRequestHandler(ciImage: ciImage).perform([request])
    }
}
```
This gives you:

* a clean UI

* photos picker

* instant on-device inference

üß™ **8. Build This Task (End-of-Module Challenge)**

Add one enhancement:

‚úî Add a VoiceOver-friendly summary

When a result appears, read aloud:

```
"Prediction: espresso, 92 percent confidence."
```

Use:

```
UIAccessibility.post(notification: .announcement, argument: label)
```

This builds empathy + accessibility into your AI UX.

üéâ **Module 3 Complete**

You now have:

Android on-device inference (Module 2)

iOS on-device inference (Module 3)

Your course now serves both ecosystems, side-by-side - a big milestone.