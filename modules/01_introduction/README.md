**üìò Module 1 - Understanding On-Device AI**
What it is, why it matters, and your first hands-on demo

Welcome to Module 1 of the On-Device AI Course - the foundation for everything you‚Äôll build next.

This module helps you understand what on-device AI really is, why it‚Äôs suddenly exploding in relevance, and how to run your first on-device AI classifier in under 5 minutes.

Let‚Äôs get started.

üéØ **Learning Goals**

By the end of this module, you‚Äôll be able to explain:

* What on-device AI means

* How it differs from cloud AI

* Why mobile hardware now supports real ML inference

* The advantages (speed, privacy, offline, reliability)

* The tradeoffs (model size, constraints)

* The basic inference flow: Image ‚Üí Model ‚Üí Output

And you‚Äôll run your first lightweight classifier using a pre-built demo.

üß† **1. What Is On-Device AI?**

On-device AI means running machine learning models locally on the phone instead of calling a remote server.

Examples you already use:

* iPhone photos auto-categorizing your pictures

* Pixel phones transcribing calls live

* WhatsApp doing client-side encryption and safety checks

* Offline translation

* Snapchat filters

* On-device spam detection

All of these use local ML inference.

‚úî No server

‚úî No network

‚úî No data leaves your device

‚úî Results in milliseconds

This is why on-device AI is having a moment now.

‚ö° **2. Why Now? (Hardware + Software Shift)**

For years, phones weren‚Äôt strong enough to run ML models efficiently.

Today:

iPhones

* 16-core Apple Neural Engine (ANE)

* Highly optimized Core ML runtime

* Unified memory architecture

Android

* NNAPI and GPU/Hexagon delegates

* TensorFlow Lite optimized for mobile ops

* NPU (Neural Processing Unit) hardware on flagship chips

This hardware is designed specifically for ML inference.
That‚Äôs why models that used to require servers can now run instantly on-device.

‚òÅÔ∏è vs üì± **3. Cloud AI vs On-Device AI**

| Cloud AI             | On-Device AI                 |
| -------------------- | ---------------------------- |
| Requires network     | Works offline                |
| Great for large LLMs | Great for fast, small models |
| Privacy concerns     | Data stays on device         |
| High latency         | ~50ms latency                |
| Expensive to scale   | Free after install           |
| Centralized          | Personalized                 |


Neither replaces the other.
Modern apps combine both.

üîÑ **4. The Basic Inference Flow**

No matter the platform (iOS / Android), the flow looks like this:

Image ‚Üí Preprocessing ‚Üí Model ‚Üí Probabilities ‚Üí Label

Step-by-step:

1. User selects a photo

2. App converts image ‚Üí pixel buffer / tensor

3. Model runs locally (Core ML / TFLite)

4. Output is a list of labels + confidence

5. App shows the result

Everything happens inside the device.
Nothing leaves the phone.

üñº **5. Your First Demo (Ready to Run)**

Below are starter apps you can run immediately.

iOS Demo (Core ML)

Folder:

``` modules/01_introduction/demos/ios/ ```


This includes:

* basic SwiftUI app

* MobileNetV2.mlmodel

* photo picker

* result label

To run:

1. Open Xcode

2. Run on Simulator or device

3. Choose any photo

4. See instant prediction

Android Demo (TensorFlow Lite)

Folder:

``` modules/01_introduction/demos/android/ ```


This includes:

* Jetpack Compose app

* TFLite MobileNetV1 model

* photo picker

* simple output label

To run:

1. Open Android Studio

2. Install on device/simulator

3. Pick a photo

4. Get instant classification

üß™ **6. Hands-On Task ("Build This")**

To complete this module, do this:

‚úî Add a confidence score (percentage)

‚úî Change the UI colors based on confidence

‚úî Add a small ‚ÄúAnalyzing‚Ä¶‚Äù loading indicator

These tiny additions will help you prepare for Module 4 (AI UX).

üìö **7. Further Reading (Optional)**

If you want to go deeper:

[Apple: Core ML overview](https://developer.apple.com/machine-learning/core-ml/)

[Google: TensorFlow Lite basics](https://android.googlesource.com/platform/external/tensorflow/+/ec63214f098a2bfc87b628219ad0718750d4e930/tensorflow/lite/g3doc/guide/get_started.md)

Qualcomm: Understanding NPUs

EfficientNet: modern image classification



üå± **8. What‚Äôs Next (Module 2 & 3)**

You now understand:

what on-device AI is,

why it matters,

how the inference pipeline works,

and you‚Äôve run your first demo.

Next, we go platform-specific.
